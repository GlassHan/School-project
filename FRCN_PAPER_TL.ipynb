{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fc2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import time\n",
    "import utils_ObjectDetection as utils\n",
    "from torch.utils.data import BatchSampler,SequentialSampler,RandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import math \n",
    "import ITB_RECALL_PRECISION_F1 as rp_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82ac59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            \n",
    "            if score > threshold : \n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "    return preds\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes,train_layer):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,trainable_backbone_layers=train_layer)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_image_from_output(img, annotation):\n",
    "    \n",
    "    img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for idx in range(len(annotation[\"boxes\"])):\n",
    "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n",
    "\n",
    "        if annotation['labels'][idx] == 1 :\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none')\n",
    "        \n",
    "        elif annotation['labels'][idx] == 2 :\n",
    "            \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "        elif annotation['labels'][idx] == 3 :\n",
    "        \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
    "        else:\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3732c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class simpleDataset(object):\n",
    "\n",
    "\n",
    "    def __init__(self, dataset,resize,color, img_foler_path,transforms=None):\n",
    "    #             self.root = root\n",
    "            self.transforms = transforms\n",
    "            self.adnoc = dataset\n",
    "            self.ids = dataset.index\n",
    "            self. filenames=dataset['path'].to_list()\n",
    "            self.resize =resize\n",
    "            self.color=color\n",
    "            self.img_foler_path=img_foler_path\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        adnoc_df = self.adnoc\n",
    "        # Image ID ,폴더에서 index번째 \n",
    "        img_id = self.ids[index]\n",
    "\n",
    "       # List: get annotation id from coco, index번째 annotation가져오기 [[1,2,3,4],[5,6,7,8]]\n",
    "        annotation = adnoc_df['box'][img_id]\n",
    "\n",
    "\n",
    "        # open the input image, 흑백=L , 단색=1\n",
    "        img= Image.open(str(self.img_foler_path)+str(adnoc_df.loc[img_id]['path'])).convert(self.color)\n",
    "        img= img.resize((int(img.width / self.resize), int(img.height / self.resize)))\n",
    "      \n",
    "        # number of objects in the image\n",
    "        num_objs = len(annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        areas = []\n",
    "        label = []\n",
    "        for i in range(num_objs):\n",
    "\n",
    "            xmin = annotation[i][0]\n",
    "            ymin = annotation[i][1]\n",
    "            xmax = xmin + annotation[i][2]\n",
    "            ymax = ymin + annotation[i][3]\n",
    "            l=annotation[i][4]\n",
    "            area=annotation[i][2]*annotation[i][3]\n",
    "\n",
    "            boxes.append([xmin/self.resize, ymin/self.resize, xmax/self.resize, ymax/self.resize])\n",
    "          \n",
    "            label.append(l)\n",
    "            areas.append(area)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        #areas= box size = width * height\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(label, dtype=torch.int64)\n",
    "\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "\n",
    "    # the total number of samples (optional)\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7f7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class uncertainty_sampling:\n",
    "    def __init__(self,model,confidence,batch,train_data_loader):\n",
    "        self.model=model\n",
    "        self.confidence=confidence\n",
    "        self.batch=batch\n",
    "        self.train_data_loader=train_data_loader\n",
    "\n",
    "\n",
    "    def img_uncertainty(self,model,ascend):\n",
    "        '''\n",
    "        내림차순, 큰 수 부터\n",
    "        ascend=False\n",
    "        오름차순, 작은 수 부터\n",
    "        ascend=True \n",
    "        \n",
    "        '''\n",
    "        img_id=[]\n",
    "        pred_label=[]\n",
    "        pred_scores=[]\n",
    "        img_uncertianty=[]\n",
    "        labels = []\n",
    "\n",
    "        print('confidence=',self.confidence)\n",
    "        for im, annot in tqdm(self.train_data_loader, position = 0, leave = True):\n",
    "            im = list(img.to(device) for img in im)\n",
    "            #test data 이미지 id 저장\n",
    "            img_id.append(annot[0]['image_id'].item())\n",
    "\n",
    "            for t in annot:\n",
    "                labels += t['labels']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_adj = make_prediction(model, im, self.confidence)\n",
    "                preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "\n",
    "                #예측 label 저장\n",
    "                pred_label.append(preds_adj[0]['labels'].tolist())\n",
    "\n",
    "                #예측 score 저장\n",
    "                pred_scores.append(preds_adj[0]['scores'].tolist())\n",
    "                img_uncertianty.append(np.mean(preds_adj[0]['scores'].tolist()))\n",
    "\n",
    "        model_pred=pd.DataFrame(img_id,columns=['img_id'])\n",
    "        model_pred['label']=pred_label\n",
    "        model_pred['score']=pred_scores\n",
    "        model_pred['uncertatinty']=img_uncertianty\n",
    "\n",
    "\n",
    "        model_pred.sort_values(by=['uncertatinty'],ascending=ascend,inplace=True)\n",
    "        return model_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self,train,ascend):\n",
    "    \n",
    "        model_pred=self.img_uncertainty(model,ascend)\n",
    "          \n",
    "        if len(train)<self.batch:\n",
    "            sample=train\n",
    "            train_rest=[]\n",
    "            train_rest_data_loader=[]\n",
    "        else:\n",
    "            #상위 10개 data 선택\n",
    "            sample_idx=model_pred[:self.batch]['img_id'].tolist()\n",
    "            sample=train.loc[sample_idx]\n",
    "            train_rest=train.drop(sample.index)\n",
    "            \n",
    "            train_rest_dataset = simpleDataset(dataset=train_rest,\n",
    "                                  resize=4,\n",
    "                                  color='L',\n",
    "                                  img_foler_path='D:/OBJ_PAPER/itb_p_500/',\n",
    "                                  transforms=get_transform())\n",
    "\n",
    "            train_rest_data_loader = torch.utils.data.DataLoader(train_rest_dataset,\n",
    "                                                  batch_size=1,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=0,\n",
    "                                                  collate_fn=collate_fn)\n",
    "\n",
    "        sample_dataset = simpleDataset(dataset=sample,\n",
    "                                  resize=4,\n",
    "                                  color='L',\n",
    "                                  img_foler_path='D:/OBJ_PAPER/itb_p_500/',\n",
    "                                  transforms=get_transform())\n",
    "\n",
    "        sample_data_loader = torch.utils.data.DataLoader(sample_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0,\n",
    "                                              collate_fn=collate_fn)\n",
    "        \n",
    "      \n",
    "        \n",
    "        return sample,train_rest,train_rest_data_loader,sample_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ccc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_RPA(test_data_loader,model,param,iteration):\n",
    "    \n",
    "    model = get_model_instance_segmentation(4,5)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(param))\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    confi=[0.5]\n",
    "    RECALL=[]\n",
    "    PRECISION=[]\n",
    "    F1_SCORE=[]\n",
    "    for c in confi: \n",
    "        labels = []\n",
    "        preds_adj_all = []\n",
    "        annot_all = []\n",
    "        print('confidence=',c)\n",
    "        for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
    "            im = list(img.to(device) for img in im)\n",
    "\n",
    "        #     annot\n",
    "            for t in annot:\n",
    "                labels += t['labels']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_adj = make_prediction(model, im, c)\n",
    "                preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "                preds_adj_all.append(preds_adj)\n",
    "                annot_all.append(annot)\n",
    "\n",
    "        #precision=0.2\n",
    "        iou_threds=0.5\n",
    "        pr_f1=rp_f1.Precision_Recall_F1(iou_threds,preds_adj_all,annot_all)\n",
    "      \n",
    "        precision=pr_f1.PRECISION()\n",
    "        recall=pr_f1.RECALL()\n",
    "        f1_score=pr_f1.F1_SCORE()\n",
    "      \n",
    "\n",
    "        RECALL.append(recall)\n",
    "        PRECISION.append(precision)\n",
    "        F1_SCORE.append(f1_score)\n",
    "    \n",
    "    \n",
    "    print(RECALL,'\\n',PRECISION,'\\n',F1_SCORE)\n",
    "    result=pd.DataFrame(RECALL,columns=['recall_form','recall_table','recall_text'])\n",
    "    result['precision_form']=PRECISION[0][0]\n",
    "    result['precision_table']=PRECISION[0][1]\n",
    "    result['precision_text']=PRECISION[0][2]\n",
    "\n",
    "    result['f1_form']=F1_SCORE[0][0]\n",
    "    result['f1_table']=F1_SCORE[0][1]\n",
    "    result['f1_text']=F1_SCORE[0][2]\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862ae8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(train,batch):\n",
    "    if len(train)<batch:\n",
    "        sampled=train\n",
    "        train_rest=[]\n",
    "        \n",
    "    else:\n",
    "        sampled=train.sample(n=batch,random_state=42)\n",
    "        train_rest=train.drop(sampled.index)\n",
    "        \n",
    "    sample_dataset = simpleDataset(dataset=sampled,\n",
    "                                  resize=4,\n",
    "                                  color='L',\n",
    "                                  img_foler_path='D:/OBJ_PAPER/itb_p_500/',\n",
    "                                  transforms=get_transform())\n",
    "    \n",
    "    sample_data_loader = torch.utils.data.DataLoader(sample_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return sampled,train_rest,sample_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c78c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load_all(train,valid,test):\n",
    " \n",
    "    # create own Dataset\n",
    "    train_dataset = simpleDataset(dataset=train,\n",
    "                                  resize=4,\n",
    "                                  color='L',\n",
    "                                  img_foler_path='D:/OBJ_PAPER/Data/itb_p_500/',\n",
    "                                  transforms=get_transform())\n",
    "\n",
    "    valid_dataset = simpleDataset(\n",
    "                              dataset=valid,\n",
    "                              resize=4,\n",
    "                              color='L',\n",
    "                              img_foler_path='D:/OBJ_PAPER/Data/itb_p_500/',\n",
    "                              transforms=get_transform())\n",
    "\n",
    "    test_dataset = simpleDataset(\n",
    "                              dataset=test,\n",
    "                              resize=4,\n",
    "                              color='L',\n",
    "                              img_foler_path='D:/OBJ_PAPER/Data/itb_p_500/',\n",
    "                              transforms=get_transform())\n",
    "\n",
    "\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0,                                         \n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "    return train_data_loader,valid_data_loader,test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd41161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(train):\n",
    " \n",
    "    # create own Dataset\n",
    "    train_dataset = simpleDataset(dataset=train,\n",
    "                                  resize=4,\n",
    "                                  color='L',\n",
    "                                  img_foler_path='D:/OBJ_PAPER/Data/itb_p_500/',\n",
    "                                  transforms=get_transform())\n",
    "\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=0,                                         \n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    " \n",
    "\n",
    "    return train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd6b7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='D:/OBJ_PAPER/Data/3_fold_cv/'\n",
    "\n",
    "train1=pd.read_pickle(data_path+'train1.pkl')\n",
    "train2=pd.read_pickle(data_path+'train2.pkl')\n",
    "train3=pd.read_pickle(data_path+'train3.pkl')\n",
    "\n",
    "valid1=pd.read_pickle(data_path+'valid1.pkl')\n",
    "valid2=pd.read_pickle(data_path+'valid2.pkl')\n",
    "valid3=pd.read_pickle(data_path+'valid3.pkl')\n",
    "\n",
    "train1_no_table=pd.read_pickle(data_path+'train1_no_table.pkl')\n",
    "train2_no_table=pd.read_pickle(data_path+'train2_no_table.pkl')\n",
    "train3_no_table=pd.read_pickle(data_path+'train3_no_table.pkl')\n",
    "\n",
    "test=pd.read_pickle(data_path+'test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33eb931",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "- 추가학습 없이 필리핀 문서 예측하기\n",
    "- test set만 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a89cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader,valid_data_loader,test_data_loader=data_load(train3,valid3,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c87166ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.27273, 0.9403, 0.62963]\n",
      "recall [0.11321, 0.96923, 0.74843]\n",
      "[[0.11321, 0.96923, 0.74843]] \n",
      " [[0.27273, 0.9403, 0.62963]] \n",
      " [[0.16, 0.95455, 0.68391]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "train_layer=5\n",
    "iteration=1\n",
    "RESULT=pd.DataFrame()\n",
    "train_param_path='D:/OBJ_PAPER/model_param/model_fasterrcnn_bach5_p10_50_param.pt'\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes,train_layer)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('D:/OBJ_PAPER/model_param/model_fasterrcnn_bach5_p10_50_param.pt'))\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "\n",
    "result_batch=result_RPA(test_data_loader,model,train_param_path,iteration)\n",
    "RESULT=RESULT.append(result_batch,ignore_index=True)\n",
    "\n",
    "    \n",
    "RESULT.to_csv('D:/OBJ_PAPER/result/FRCN_TL_no_train_3.csv',index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7093b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 98/98 [00:12<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.0, 0.0, 0.00923]\n",
      "recall [0.0, 0.0, 0.03448]\n",
      "[[0.0, 0.0, 0.03448]] \n",
      " [[0.0, 0.0, 0.00923]] \n",
      " [[0.0, 0.0, 0.01456]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "train_layer=5\n",
    "iteration=1\n",
    "RESULT=pd.DataFrame()\n",
    "\n",
    "\n",
    "model = get_model_instance_segmentation(4,5)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "confi=[0.5]\n",
    "RECALL=[]\n",
    "PRECISION=[]\n",
    "F1_SCORE=[]\n",
    "for c in confi: \n",
    "    labels = []\n",
    "    preds_adj_all = []\n",
    "    annot_all = []\n",
    "    print('confidence=',c)\n",
    "    for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
    "        im = list(img.to(device) for img in im)\n",
    "\n",
    "    #     annot\n",
    "        for t in annot:\n",
    "            labels += t['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_adj = make_prediction(model, im, c)\n",
    "            preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "            preds_adj_all.append(preds_adj)\n",
    "            annot_all.append(annot)\n",
    "\n",
    "    #precision=0.2\n",
    "    iou_threds=0.5\n",
    "    pr_f1=rp_f1.Precision_Recall_F1(iou_threds,preds_adj_all,annot_all)\n",
    "\n",
    "    precision=pr_f1.PRECISION()\n",
    "    recall=pr_f1.RECALL()\n",
    "    f1_score=pr_f1.F1_SCORE()\n",
    "\n",
    "\n",
    "    RECALL.append(recall)\n",
    "    PRECISION.append(precision)\n",
    "    F1_SCORE.append(f1_score)\n",
    "\n",
    "\n",
    "print(RECALL,'\\n',PRECISION,'\\n',F1_SCORE)\n",
    "result=pd.DataFrame(RECALL,columns=['recall_form','recall_table','recall_text'])\n",
    "result['precision_form']=PRECISION[0][0]\n",
    "result['precision_table']=PRECISION[0][1]\n",
    "result['precision_text']=PRECISION[0][2]\n",
    "\n",
    "result['f1_form']=F1_SCORE[0][0]\n",
    "result['f1_table']=F1_SCORE[0][1]\n",
    "result['f1_text']=F1_SCORE[0][2]\n",
    "    \n",
    "\n",
    "RESULT=RESULT.append(result,ignore_index=True)\n",
    "\n",
    "    \n",
    "RESULT.to_csv('D:/OBJ_PAPER/result/FRCN_can.csv',index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4de888",
   "metadata": {},
   "source": [
    "### S사로 학습한 모델에 신규 데이터 추가 학습\n",
    "### train_layer=5, valid set = 50, test set 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b649b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_TL(save_param_path,result_path,train_data_loader,valid_data_loader,test_data_loader):\n",
    "    # 2 classes; Only target class or background\n",
    "    num_classes = 4\n",
    "    num_epochs = 100\n",
    "    train_layer=1\n",
    "    patience=10\n",
    "\n",
    "    model = get_model_instance_segmentation(num_classes,train_layer)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    #모델 저장 \n",
    "    train_param_path=save_param_path\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load('D:/OBJ_PAPER/model_param/model_fasterrcnn_bach5_p10_50_param.pt'))\n",
    "\n",
    "    # parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "    #평균 loss\n",
    "    avg_train_loss=[]\n",
    "    avg_valid_loss=[]\n",
    "\n",
    "    iteration = 0    \n",
    "\n",
    "    RESULT=pd.DataFrame()\n",
    "\n",
    "    model.train()\n",
    "    not_save_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # 모델이 학습되는 동안 trainning loss를 track\n",
    "        train_losses = []\n",
    "        # 모델이 학습되는 동안 validation loss를 track\n",
    "        valid_losses = []\n",
    "        st = time.time()\n",
    "        for imgs, annotations in train_data_loader:\n",
    "\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "            loss_dict = model(imgs, annotations)\n",
    "            #batch size=10, 10개 loss 각각 도출\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            train_losses.append(losses.item())       \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #이미지 한장당 평균 loss\n",
    "        avg_train_loss.append(np.mean(train_losses).round(5))\n",
    "\n",
    "        #validation, early_stop, save weights\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for im, annot in valid_data_loader:\n",
    "                im = list(img.to(device) for img in im)\n",
    "                annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
    "                val_loss_dict = model(im, annot)\n",
    "                val_losses = sum(val_loss for val_loss in val_loss_dict.values())\n",
    "                valid_losses.append(val_losses.item())\n",
    "\n",
    "            epoch_val_loss=np.mean(valid_losses).round(5)\n",
    "            avg_valid_loss.append(epoch_val_loss)  \n",
    "\n",
    "        fi = time.time()     \n",
    "        print('epoch:',epoch,'train_loss: ',np.mean(train_losses).round(5),'validation loss : ',np.mean(valid_losses).round(5),'time',fi-st)\n",
    "\n",
    "\n",
    "        min_val_loss=np.min(avg_valid_loss)\n",
    "        if min_val_loss>=epoch_val_loss:\n",
    "\n",
    "            torch.save(model.state_dict(),train_param_path)\n",
    "            not_save_count=0\n",
    "            print('epoch:',epoch,'save model')\n",
    "\n",
    "        else:\n",
    "            not_save_count+=1\n",
    "            model.load_state_dict(torch.load(train_param_path))\n",
    "            if not_save_count>=patience:\n",
    "                print('no more training')\n",
    "                break\n",
    "\n",
    "\n",
    "    fi = time.time()     \n",
    "    print('iteration:',iteration,'train_loss: ',np.mean(train_losses).round(5),'val_loss: ',min_val_loss.round(5),'time',fi-st)\n",
    "    model.load_state_dict(torch.load(train_param_path))\n",
    "\n",
    "    result_batch=result_RPA(test_data_loader,model,train_param_path,iteration)\n",
    "    RESULT=RESULT.append(result_batch,ignore_index=True)\n",
    "    RESULT.to_csv(result_path,index=False)\n",
    "    return RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d46c3c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss:  0.20101 validation loss :  0.25377 time 40.627991914749146\n",
      "epoch: 0 save model\n",
      "epoch: 1 train_loss:  0.13921 validation loss :  0.23564 time 40.462947845458984\n",
      "epoch: 1 save model\n",
      "epoch: 2 train_loss:  0.11229 validation loss :  0.25218 time 40.42556166648865\n",
      "epoch: 3 train_loss:  0.1135 validation loss :  0.25877 time 40.427637338638306\n",
      "epoch: 4 train_loss:  0.11607 validation loss :  0.26152 time 40.54937672615051\n",
      "epoch: 5 train_loss:  0.11749 validation loss :  0.26858 time 40.53559494018555\n",
      "epoch: 6 train_loss:  0.11459 validation loss :  0.28413 time 40.78890514373779\n",
      "epoch: 7 train_loss:  0.11594 validation loss :  0.26541 time 40.65884351730347\n",
      "epoch: 8 train_loss:  0.117 validation loss :  0.28456 time 40.584728479385376\n",
      "epoch: 9 train_loss:  0.11885 validation loss :  0.2494 time 40.62306880950928\n",
      "epoch: 10 train_loss:  0.12075 validation loss :  0.29592 time 40.61904692649841\n",
      "epoch: 11 train_loss:  0.11788 validation loss :  0.30538 time 40.62959861755371\n",
      "no more training\n",
      "iteration: 0 train_loss:  0.11788 val_loss:  0.23564 time 40.77641725540161\n",
      "confidence= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.52128, 0.0, 0.55357]\n",
      "recall [0.92453, 0.0, 0.99359]\n",
      "[[0.92453, 0.0, 0.99359]] \n",
      " [[0.52128, 0.0, 0.55357]] \n",
      " [[0.66667, 0.0, 0.71101]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall_form</th>\n",
       "      <th>recall_table</th>\n",
       "      <th>recall_text</th>\n",
       "      <th>precision_form</th>\n",
       "      <th>precision_table</th>\n",
       "      <th>precision_text</th>\n",
       "      <th>f1_form</th>\n",
       "      <th>f1_table</th>\n",
       "      <th>f1_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.92453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99359</td>\n",
       "      <td>0.52128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55357</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recall_form  recall_table  recall_text  precision_form  precision_table  \\\n",
       "0      0.92453           0.0      0.99359         0.52128              0.0   \n",
       "\n",
       "   precision_text  f1_form  f1_table  f1_text  \n",
       "0         0.55357  0.66667       0.0  0.71101  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_param_path='D:/OBJ_PAPER/model_param/FRCN_TL_valid50_layer1_train1_no_table_param.pt'\n",
    "result_path='D:/OBJ_PAPER/result/FRCN_TL_valid50_layer1_train1_no_table_result.csv'\n",
    "\n",
    "train_data_loader,valid_data_loader,test_data_loader=data_load_all(train1_no_table,valid1,test)\n",
    "RESULT=train_TL(save_param_path,result_path,train_data_loader,valid_data_loader,test_data_loader)\n",
    "RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc4b88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss:  0.20192 validation loss :  0.23116 time 40.97726821899414\n",
      "epoch: 0 save model\n",
      "epoch: 1 train_loss:  0.14075 validation loss :  0.24177 time 41.156251668930054\n",
      "epoch: 2 train_loss:  0.13638 validation loss :  0.2673 time 41.094810485839844\n",
      "epoch: 3 train_loss:  0.13855 validation loss :  0.25438 time 41.064656496047974\n",
      "epoch: 4 train_loss:  0.13997 validation loss :  0.29963 time 41.21307682991028\n",
      "epoch: 5 train_loss:  0.14256 validation loss :  0.25573 time 41.138429403305054\n",
      "epoch: 6 train_loss:  0.14507 validation loss :  0.28261 time 41.18891215324402\n",
      "epoch: 7 train_loss:  0.14624 validation loss :  0.27771 time 41.27439332008362\n",
      "epoch: 8 train_loss:  0.14193 validation loss :  0.27271 time 41.3082230091095\n",
      "epoch: 9 train_loss:  0.14382 validation loss :  0.26435 time 41.60800838470459\n",
      "epoch: 10 train_loss:  0.14777 validation loss :  0.28618 time 41.382527351379395\n",
      "no more training\n",
      "iteration: 0 train_loss:  0.14777 val_loss:  0.23116 time 41.523423194885254\n",
      "confidence= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.52222, 0.0, 0.6383]\n",
      "recall [0.88679, 0.0, 0.96154]\n",
      "[[0.88679, 0.0, 0.96154]] \n",
      " [[0.52222, 0.0, 0.6383]] \n",
      " [[0.65734, 0.0, 0.76727]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall_form</th>\n",
       "      <th>recall_table</th>\n",
       "      <th>recall_text</th>\n",
       "      <th>precision_form</th>\n",
       "      <th>precision_table</th>\n",
       "      <th>precision_text</th>\n",
       "      <th>f1_form</th>\n",
       "      <th>f1_table</th>\n",
       "      <th>f1_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.88679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96154</td>\n",
       "      <td>0.52222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6383</td>\n",
       "      <td>0.65734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recall_form  recall_table  recall_text  precision_form  precision_table  \\\n",
       "0      0.88679           0.0      0.96154         0.52222              0.0   \n",
       "\n",
       "   precision_text  f1_form  f1_table  f1_text  \n",
       "0          0.6383  0.65734       0.0  0.76727  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_param_path='D:/OBJ_PAPER/model_param/FRCN_TL_valid50_layer1_train2_no_table_param.pt'\n",
    "result_path='D:/OBJ_PAPER/result/FRCN_TL_valid50_layer1_train2_no_table_result.csv'\n",
    "\n",
    "train_data_loader,valid_data_loader,test_data_loader=data_load_all(train2_no_table,valid2,test)\n",
    "RESULT=train_TL(save_param_path,result_path,train_data_loader,valid_data_loader,test_data_loader)\n",
    "RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f70c6a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss:  0.19544 validation loss :  0.28476 time 40.925448179244995\n",
      "epoch: 0 save model\n",
      "epoch: 1 train_loss:  0.12515 validation loss :  0.37451 time 40.968366622924805\n",
      "epoch: 2 train_loss:  0.12729 validation loss :  0.35135 time 41.27254247665405\n",
      "epoch: 3 train_loss:  0.1256 validation loss :  0.39794 time 41.041789054870605\n",
      "epoch: 4 train_loss:  0.13407 validation loss :  0.37598 time 40.95479655265808\n",
      "epoch: 5 train_loss:  0.12797 validation loss :  0.35521 time 41.18832588195801\n",
      "epoch: 6 train_loss:  0.13161 validation loss :  0.37056 time 40.964760065078735\n",
      "epoch: 7 train_loss:  0.1311 validation loss :  0.38517 time 41.02539324760437\n",
      "epoch: 8 train_loss:  0.13212 validation loss :  0.37682 time 41.01503014564514\n",
      "epoch: 9 train_loss:  0.13429 validation loss :  0.4118 time 41.06053948402405\n",
      "epoch: 10 train_loss:  0.13273 validation loss :  0.39545 time 40.991140604019165\n",
      "no more training\n",
      "iteration: 0 train_loss:  0.13273 val_loss:  0.28476 time 41.13447952270508\n",
      "confidence= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.39837, 0.0, 0.4416]\n",
      "recall [0.92453, 0.0, 0.99359]\n",
      "[[0.92453, 0.0, 0.99359]] \n",
      " [[0.39837, 0.0, 0.4416]] \n",
      " [[0.55681, 0.0, 0.61144]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall_form</th>\n",
       "      <th>recall_table</th>\n",
       "      <th>recall_text</th>\n",
       "      <th>precision_form</th>\n",
       "      <th>precision_table</th>\n",
       "      <th>precision_text</th>\n",
       "      <th>f1_form</th>\n",
       "      <th>f1_table</th>\n",
       "      <th>f1_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.92453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99359</td>\n",
       "      <td>0.39837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4416</td>\n",
       "      <td>0.55681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recall_form  recall_table  recall_text  precision_form  precision_table  \\\n",
       "0      0.92453           0.0      0.99359         0.39837              0.0   \n",
       "\n",
       "   precision_text  f1_form  f1_table  f1_text  \n",
       "0          0.4416  0.55681       0.0  0.61144  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_param_path='D:/OBJ_PAPER/model_param/FRCN_TL_valid50_layer1_train3_no_table_param.pt'\n",
    "result_path='D:/OBJ_PAPER/result/FRCN_TL_valid50_layer1_train3_no_table_result.csv'\n",
    "\n",
    "train_data_loader,valid_data_loader,test_data_loader=data_load_all(train3_no_table,valid3,test)\n",
    "RESULT=train_TL(save_param_path,result_path,train_data_loader,valid_data_loader,test_data_loader)\n",
    "RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a50d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
